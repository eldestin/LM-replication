{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:14.901204Z",
     "iopub.status.busy": "2022-06-10T15:14:14.900484Z",
     "iopub.status.idle": "2022-06-10T15:14:23.001047Z",
     "shell.execute_reply": "2022-06-10T15:14:23.000262Z",
     "shell.execute_reply.started": "2022-06-10T15:14:14.901122Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.cuda import amp\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from collections import Counter\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Dataset\n",
    "Notice that in Machine translation task, each token in both source language and target language should be create a dictionary mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:23.003694Z",
     "iopub.status.busy": "2022-06-10T15:14:23.002976Z",
     "iopub.status.idle": "2022-06-10T15:14:23.016726Z",
     "shell.execute_reply": "2022-06-10T15:14:23.015966Z",
     "shell.execute_reply.started": "2022-06-10T15:14:23.003654Z"
    }
   },
   "outputs": [],
   "source": [
    "class Build_vocabulary(object):\n",
    "    '''\n",
    "    Here we need to bulid a vocabulary for mapping\n",
    "    '''\n",
    "    def __init__(self, tokens = None, min_freq = 0, special_tokens = None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if special_tokens is None:\n",
    "            special_tokens = []\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "        counter = Counter(tokens)\n",
    "        # sort by frequency\n",
    "        self.freq = sorted(counter.items(), key = lambda x: x[1], reverse=True)\n",
    "        # set special token\n",
    "        self.idx_to_token = [\"<unk>\"] + special_tokens\n",
    "        self.token_to_id = {token: ids for ids, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self.freq:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_id:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_id[token] = len(self.idx_to_token) - 1\n",
    "                \n",
    "    #build some internal property\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        '''\n",
    "        Return the token:ids for each input token in dict\n",
    "        '''\n",
    "        if not isinstance(token, (list, tuple)):\n",
    "            return self.token_to_id.get(token, 0)\n",
    "        return [self.__getitem__(token_) for token_ in token]\n",
    "    \n",
    "    def indices_to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the French - english translation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:23.019640Z",
     "iopub.status.busy": "2022-06-10T15:14:23.018940Z",
     "iopub.status.idle": "2022-06-10T15:14:23.028944Z",
     "shell.execute_reply": "2022-06-10T15:14:23.028262Z",
     "shell.execute_reply.started": "2022-06-10T15:14:23.019603Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    with open(\"./fra-eng/fra.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        text = text.replace('\\u202f', ' ').replace('\\xa0', ' ').lower()\n",
    "    def no_space(char, prev_char):\n",
    "        return char in set(',.!?') and prev_char != \" \"\n",
    "    out = []\n",
    "    for i, char in enumerate(text):\n",
    "        if i>0 and no_space(char, text[i-1]):\n",
    "            out.append(' '+char)\n",
    "        else:\n",
    "            out.append(char)\n",
    "    text = \"\".join(out)\n",
    "    # Tokenization\n",
    "    english_word, french_word = [], []\n",
    "    for i, sentence in enumerate(text.split(\"\\n\")):\n",
    "        # split by \\t\n",
    "        result = sentence.split(\"\\t\")\n",
    "        if len(result) == 2:\n",
    "            english_word.append(result[0].split(\" \"))\n",
    "            french_word.append(result[1].split(\" \"))\n",
    "    return english_word, french_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:23.031567Z",
     "iopub.status.busy": "2022-06-10T15:14:23.031108Z",
     "iopub.status.idle": "2022-06-10T15:14:31.424902Z",
     "shell.execute_reply": "2022-06-10T15:14:31.423996Z",
     "shell.execute_reply.started": "2022-06-10T15:14:23.031532Z"
    }
   },
   "outputs": [],
   "source": [
    "english_word, french_word = read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, also add special tokens: \\<pad>, \\<bos>, \\<eos>, and create the mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding and truncation, Notice that padding is add \\<pad> here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.431986Z",
     "iopub.status.busy": "2022-06-10T15:14:31.429541Z",
     "iopub.status.idle": "2022-06-10T15:14:31.439348Z",
     "shell.execute_reply": "2022-06-10T15:14:31.438630Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.431946Z"
    }
   },
   "outputs": [],
   "source": [
    "def padding_truncation(tokens, max_lens, padding_token):\n",
    "    if len(tokens) > max_lens:\n",
    "        return tokens[:max_lens]\n",
    "    return tokens + [padding_token]*(max_lens - len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.441626Z",
     "iopub.status.busy": "2022-06-10T15:14:31.440938Z",
     "iopub.status.idle": "2022-06-10T15:14:31.449901Z",
     "shell.execute_reply": "2022-06-10T15:14:31.449152Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.441588Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_array(tokens, dic, max_length):\n",
    "    '''\n",
    "    This function build the array of each token\n",
    "    '''\n",
    "    tokens_mapping = [dic[token] for token in tokens]\n",
    "    tokens_mapping = [token + [dic[\"<eos>\"]] for token in tokens_mapping]\n",
    "    # add padding, truncation\n",
    "    tensor = torch.tensor([padding_truncation(token, max_length, dic[\"<pad>\"]) for token in tokens_mapping])\n",
    "    valid_len = (tensor != dic[\"<pad>\"]).type(torch.int32).sum(1)\n",
    "    return tensor, valid_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.452287Z",
     "iopub.status.busy": "2022-06-10T15:14:31.451529Z",
     "iopub.status.idle": "2022-06-10T15:14:31.461164Z",
     "shell.execute_reply": "2022-06-10T15:14:31.460344Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.452252Z"
    }
   },
   "outputs": [],
   "source": [
    "def processing_french_english_dataset(batch_size, max_length):\n",
    "    english_word, french_word = read_dataset()\n",
    "    english_mapping = Build_vocabulary(english_word, min_freq=2, \n",
    "                                   special_tokens=[\"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "    french_mapping = Build_vocabulary(french_word, min_freq=2, \n",
    "                                   special_tokens=[\"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "    english_array, english_valid_len = build_array(english_word, english_mapping, max_length)\n",
    "    french_array, french_valid_len = build_array(french_word, french_mapping, max_length)\n",
    "    dataset = torch.utils.data.TensorDataset(*(english_array, english_valid_len, french_array, french_valid_len))\n",
    "    data_iter = torch.utils.data.DataLoader(dataset,batch_size = batch_size, shuffle = True)\n",
    "    return data_iter, english_mapping, french_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "We totally have 3 parts:\n",
    "1. Multi-head self-attention\n",
    "2. Position-wise fully connected layer\n",
    "3. Add & norm\n",
    "4. Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention\n",
    "Given query: $\\mathbf{q} \\in \\mathbb{R}^{d_q}$,\n",
    "key$\\mathbf{k} \\in \\mathbb{R}^{d_k}$, \n",
    "value$\\mathbf{v} \\in \\mathbb{R}^{d_v}$, the calculation method of each attention head is:$$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, for each $h_i$, given a concatenation of them and finally passing a learnable variable $W_o$ to project the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.465554Z",
     "iopub.status.busy": "2022-06-10T15:14:31.464964Z",
     "iopub.status.idle": "2022-06-10T15:14:31.479691Z",
     "shell.execute_reply": "2022-06-10T15:14:31.478843Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.465510Z"
    }
   },
   "outputs": [],
   "source": [
    "def masked_attention_softmax(attention_score, valid_len=None):\n",
    "    '''\n",
    "    This function provide the masked attention caculation result when passing\n",
    "    the softmax normalization.\n",
    "    valid len is a tensor, where means each\n",
    "    '''\n",
    "    if valid_len is None:\n",
    "        return nn.functional.softmax(attention_score, dim = -1)\n",
    "    else:\n",
    "        # take a multiplication to mask\n",
    "        if valid_len.dim() == 1:\n",
    "            # if 1 dimension\n",
    "            # repeat for each dimension\n",
    "            valid_len = torch.repeat_interleave(valid_len, attention_score.shape[1])\n",
    "        else:\n",
    "            valid_len = valid_len.reshape(-1)\n",
    "        #print(\"valid length\", valid_len[:,None])\n",
    "        max_length = attention_score.shape[-1]\n",
    "        tmp_att_score = attention_score.reshape(-1, attention_score.shape[-1])\n",
    "        # create masking, here None expand dimension on dim 0 or -1\n",
    "#         print(torch.arange((max_length), dtype = torch.float32, \n",
    "#                             device = attention_score.device)[None,:])\n",
    "        # use 1*4, 4*1 to broadcast result \n",
    "        mask = torch.arange((max_length), dtype = torch.float32, \n",
    "                            device = attention_score.device)[None,:] < valid_len[:,None]\n",
    "        #print(mask)\n",
    "        tmp_att_score[~mask] = -1e6\n",
    "        return nn.functional.softmax(tmp_att_score.reshape(attention_score.shape), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.484376Z",
     "iopub.status.busy": "2022-06-10T15:14:31.483675Z",
     "iopub.status.idle": "2022-06-10T15:14:31.495558Z",
     "shell.execute_reply": "2022-06-10T15:14:31.494728Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.484335Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dot_product_attention(nn.Module):\n",
    "    def __init__(self, dropout_rate, **kwargs):\n",
    "        super(Dot_product_attention,self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.attention_weight = None\n",
    "    def forward(self, query, key, value, valid_len = None):\n",
    "        '''\n",
    "        Query * key.T/ sqrt d * value\n",
    "        '''\n",
    "        mult = torch.bmm(query, key.transpose(1,2)) / math.sqrt(query.shape[-1])\n",
    "        self.attention_weight = masked_attention_softmax(mult, valid_len)\n",
    "        return torch.bmm(self.dropout(self.attention_weight), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.500493Z",
     "iopub.status.busy": "2022-06-10T15:14:31.499736Z",
     "iopub.status.idle": "2022-06-10T15:14:31.513547Z",
     "shell.execute_reply": "2022-06-10T15:14:31.512390Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.500458Z"
    }
   },
   "outputs": [],
   "source": [
    "def parallel_calculate_multihead(input_, num_heads, reverse = False):\n",
    "    '''\n",
    "    This function provide a parallel computation of multiple attention heads\n",
    "    '''\n",
    "    if not reverse:\n",
    "        # (bs, num_key,value pair, hidden_size)\n",
    "        # change to (bs, num_key,value pair, num_heads, hidden_size/num_heads)\n",
    "        input_ = input_.reshape(input_.shape[0], input_.shape[1], num_heads, -1)\n",
    "        # change shape to(bs, num_Head, num_key,value pair, hidden/num_heads)\n",
    "        input_ = input_.permute(0,2,1,3)\n",
    "        # return shape: (bs * num_head, num_key, value pair, hidden/num_head)\n",
    "        return input_.reshape(-1, input_.shape[2], input_.shape[3])\n",
    "    else:\n",
    "        # change back to (bs, num_heads, num_key, value pair, hidden/num_heads)\n",
    "        input_ = input_.reshape(-1, num_heads, input_.shape[1], input_.shape[2])\n",
    "        input_ = input_.permute(0,2,1,3)\n",
    "        return input_.reshape(input_.shape[0], input_.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.515389Z",
     "iopub.status.busy": "2022-06-10T15:14:31.514589Z",
     "iopub.status.idle": "2022-06-10T15:14:31.528839Z",
     "shell.execute_reply": "2022-06-10T15:14:31.528094Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.515355Z"
    }
   },
   "outputs": [],
   "source": [
    "class Multi_head_attention(nn.Module):\n",
    "    def __init__(self, key_size, value_size, query_size, num_hidden_size, num_heads,\n",
    "                dropout_rate, bias = False, **kwargs):\n",
    "        super(Multi_head_attention,self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = Dot_product_attention(dropout_rate)\n",
    "        #---------------- 3 learnable variables --------------------------\n",
    "        self.W_query = nn.Linear(query_size, num_hidden_size, bias=bias)\n",
    "        self.W_key = nn.Linear(key_size, num_hidden_size, bias=bias)\n",
    "        self.W_value = nn.Linear(value_size, num_hidden_size, bias=bias)\n",
    "        # ------------------- Projection of final hiddens\n",
    "        self.W_output = nn.Linear(num_hidden_size, num_hidden_size, bias=bias)\n",
    "    \n",
    "    def forward(self, query, key, value, valid_len):\n",
    "        query = parallel_calculate_multihead(self.W_query(query), self.num_heads)\n",
    "        key = parallel_calculate_multihead(self.W_key(key), self.num_heads)\n",
    "        value = parallel_calculate_multihead(self.W_query(value), self.num_heads)\n",
    "        if valid_len is not None:\n",
    "            valid_len = torch.repeat_interleave(valid_len, repeats = self.num_heads,\n",
    "                                               dim = 0)\n",
    "        output = self.attention(query, key, value, valid_len)\n",
    "        concat = parallel_calculate_multihead(output, self.num_heads, reverse = True)\n",
    "        # (bs, num_query, num_hidden)\n",
    "        return self.W_output(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForward Network\n",
    "just a basic 2-layer MLP with relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.530661Z",
     "iopub.status.busy": "2022-06-10T15:14:31.530093Z",
     "iopub.status.idle": "2022-06-10T15:14:31.541252Z",
     "shell.execute_reply": "2022-06-10T15:14:31.540424Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.530624Z"
    }
   },
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, **kwargs):\n",
    "        super(FFN, self).__init__(**kwargs)\n",
    "        self.ln1 = nn.Linear(input_size, hidden_size)\n",
    "        self.act = nn.ReLU()\n",
    "        self.ln2 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, input_):\n",
    "        return self.ln2(self.act(self.ln1(input_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add&Norm\n",
    "By applying residual(short cut) here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.542792Z",
     "iopub.status.busy": "2022-06-10T15:14:31.542250Z",
     "iopub.status.idle": "2022-06-10T15:14:31.556772Z",
     "shell.execute_reply": "2022-06-10T15:14:31.556072Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.542755Z"
    }
   },
   "outputs": [],
   "source": [
    "class Add_and_norm(nn.Module):\n",
    "    def __init__(self, norm_shape, dropout_rate, **kwargs):\n",
    "        super(Add_and_norm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(norm_shape)\n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "        return self.layer_norm(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.557971Z",
     "iopub.status.busy": "2022-06-10T15:14:31.557637Z",
     "iopub.status.idle": "2022-06-10T15:14:31.570041Z",
     "shell.execute_reply": "2022-06-10T15:14:31.569284Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.557937Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough `P`\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1) / torch.pow(10000, torch.arange(\n",
    "            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder\n",
    "Transformer encoder needs multihead attention, AddNorm, FeedForward, AddNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.571738Z",
     "iopub.status.busy": "2022-06-10T15:14:31.571137Z",
     "iopub.status.idle": "2022-06-10T15:14:31.583130Z",
     "shell.execute_reply": "2022-06-10T15:14:31.582356Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.571702Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, query_size, key_size, value_size, num_heads, num_hiddens, \n",
    "                norm_shape, num_input_size, num_hidden_size, num_output_size, \n",
    "                dropout_rate, bias=False, **kwargs):\n",
    "        '''\n",
    "        input params:\n",
    "            query_size: query input dim\n",
    "            key_size: key input dim\n",
    "            value_size: value input dim\n",
    "            num_heads: number of attention heads\n",
    "            num_hidden: number of hidden size per each attention projection\n",
    "            norm_shape: normalize shape of layer normalization\n",
    "            num_input_size: input dim of ffn\n",
    "            num_hidden_size: hidden dim of ffn\n",
    "            num_output_size: output dim of ffn\n",
    "            dropout_rate: drop out rate\n",
    "            bias: whether use bias\n",
    "        '''\n",
    "        assert num_input_size == num_hiddens, \"Input FFN size should be equal to hidden size of Attention Output\"\n",
    "        assert num_output_size == num_hiddens, \"Output FFN size should be equal to hidden size of Attention Output\"\n",
    "        assert num_hiddens % num_heads == 0, \"Hidden size should be totally divided by num of heads\"\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.attention = Multi_head_attention(key_size, value_size, query_size,\n",
    "                                             num_hiddens,num_heads, dropout_rate,bias)\n",
    "        self.add_norm = Add_and_norm(norm_shape, dropout_rate)\n",
    "        self.ffn = FFN(num_input_size, num_hidden_size, num_output_size)\n",
    "        self.add_norm2 = Add_and_norm(norm_shape, dropout_rate)\n",
    "        \n",
    "    def forward(self, input_, valid_len):\n",
    "        # self attention\n",
    "        #print(self.attention(input_, input_, input_, valid_len).shape)\n",
    "        Res1 = self.add_norm(input_, self.attention(input_, input_, input_, valid_len))\n",
    "        Res2 = self.add_norm2(Res1, self.ffn(Res1))\n",
    "        return Res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.585080Z",
     "iopub.status.busy": "2022-06-10T15:14:31.584421Z",
     "iopub.status.idle": "2022-06-10T15:14:31.600172Z",
     "shell.execute_reply": "2022-06-10T15:14:31.599388Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.585023Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    '''\n",
    "    Here is the encoder of Transformer\n",
    "    '''\n",
    "    def __init__(self, num_words,query_size, key_size, value_size, num_heads, num_hiddens, \n",
    "                norm_shape, num_input_size, num_hidden_size, num_output_size, num_layers,\n",
    "                dropout_rate, bias=False, **kwargs):\n",
    "        '''\n",
    "        input params:\n",
    "            num_Words: number of words in dic\n",
    "            query_size: query input dim\n",
    "            key_size: key input dim\n",
    "            value_size: value input dim\n",
    "            num_heads: number of attention heads\n",
    "            num_hidden: number of hidden size per each attention projection\n",
    "            norm_shape: normalize shape of layer normalization\n",
    "            num_input_size: input dim of ffn\n",
    "            num_hidden_size: hidden dim of ffn\n",
    "            num_output_size: output dim of ffn\n",
    "            num_layers: How many encoder blocks\n",
    "            dropout_rate: drop out rate\n",
    "            bias: whether use bias\n",
    "        '''\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        # token embedding\n",
    "        # to save\n",
    "        self.attention_weight = None\n",
    "        self.embedding_matrix = nn.Embedding(num_words, num_hiddens)\n",
    "        # positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(num_hiddens, dropout_rate)\n",
    "        self.encoder_blk = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.encoder_blk.add_module(\n",
    "            \"Encoder_block \"+ str(i), Encoder(query_size, key_size, value_size, num_heads,\n",
    "                                             num_hiddens, norm_shape, num_input_size, num_hidden_size,\n",
    "                                             num_output_size,dropout_rate, bias))\n",
    "            \n",
    "    def forward(self, input_, valid_len):\n",
    "        # rescaled by multiplying sqrt hidden dimension\n",
    "        X = self.positional_encoding(self.embedding_matrix(input_)*math.sqrt(self.num_hiddens))\n",
    "        self.attention_weight = [None] * len(self.encoder_blk)\n",
    "        # pass each blk\n",
    "        for i, blk in enumerate(self.encoder_blk):\n",
    "            X = blk(X,valid_len)\n",
    "            # record attention weight on Dot product attention classes\n",
    "            self.attention_weight[i] = blk.attention.attention.attention_weight\n",
    "        # output shape: bs, num_k,q,v, per batch, num_hiddens\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder\n",
    "Different from Transformer Encoder, append a masked attention before the Encoder layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.602070Z",
     "iopub.status.busy": "2022-06-10T15:14:31.601562Z",
     "iopub.status.idle": "2022-06-10T15:14:31.622807Z",
     "shell.execute_reply": "2022-06-10T15:14:31.622106Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.602019Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,query_size, key_size, value_size, num_heads, num_hiddens, \n",
    "                norm_shape, num_input_size, num_hidden_size, num_output_size,\n",
    "                dropout_rate, ith, bias=False,**kwargs):\n",
    "        '''\n",
    "        input params:\n",
    "            query_size: query input dim\n",
    "            key_size: key input dim\n",
    "            value_size: value input dim\n",
    "            num_heads: number of attention heads\n",
    "            num_hidden: number of hidden size per each attention projection\n",
    "            norm_shape: normalize shape of layer normalization\n",
    "            num_input_size: input dim of ffn\n",
    "            num_hidden_size: hidden dim of ffn\n",
    "            num_output_size: output dim of ffn\n",
    "            num_layers: How many encoder blocks\n",
    "            dropout_rate: drop out rate\n",
    "            ith: ith layer of encoder\n",
    "            bias: whether use bias\n",
    "        '''\n",
    "        super(Decoder,self).__init__(**kwargs)\n",
    "        assert num_input_size == num_hiddens, \"Input FFN size should be equal to hidden size of Attention Output\"\n",
    "        assert num_output_size == num_hiddens, \"Output FFN size should be equal to hidden size of Attention Output\"\n",
    "        assert num_hiddens % num_heads == 0, \"Hidden size should be totally divided by num of heads\"\n",
    "        self.ith = ith\n",
    "        self.attention_mask = Multi_head_attention(key_size, value_size, query_size,\n",
    "                                             num_hiddens,num_heads, dropout_rate,bias)\n",
    "        self.add_norm = Add_and_norm(norm_shape, dropout_rate)\n",
    "        self.attention2 = Multi_head_attention(key_size, value_size, query_size,\n",
    "                                             num_hiddens,num_heads, dropout_rate,bias)\n",
    "        self.add_norm2 = Add_and_norm(norm_shape, dropout_rate)\n",
    "        self.ffn = FFN(num_input_size, num_hidden_size, num_output_size)\n",
    "        self.add_norm3 = Add_and_norm(norm_shape, dropout_rate)\n",
    "        self.training = True\n",
    "    def forward(self, input_, ith_state, valid_len, state_ls):\n",
    "        '''\n",
    "        Take self attention as k,q,v masked attention,\n",
    "        Take ith_state as k,v, with query as output of masked attention output\n",
    "        Notice that only training seq2seq, we know the time step. When prediction,\n",
    "        We only predict output sequence token by token, which means only generated tokens \n",
    "        can be used in decoder self-attention\n",
    "        '''\n",
    "        # means training\n",
    "        if state_ls[self.ith] is None:\n",
    "            # means here is the initial state\n",
    "            # record key and values of encoder output\n",
    "            keys, values = input_, input_\n",
    "        # means validation\n",
    "        else:\n",
    "            # concat on time steps, which means the previous nth steps \n",
    "            keys, values = torch.cat([state_ls[self.ith], input_], axis = 1),torch.cat([state_ls[self.ith], input_], axis = 1)\n",
    "            #print(\"After concatenation\", keys.shape)\n",
    "        # record to the state ls for next decoder block \n",
    "        state_ls[self.ith] = keys\n",
    "        if self.training:\n",
    "            #num kqv means the total len of token, also the step\n",
    "            bs, num_kqv, _ = input_.shape\n",
    "            # make sure only known the calculated output instead all the token\n",
    "            decoder_valid_lens = torch.arange(1, num_kqv+1, device = input_.device).repeat(bs, 1)\n",
    "        else:\n",
    "            decoder_valid_lens = None\n",
    "        # add ith_state attention with output of encoder\n",
    "        # All query, key, value comes from last decoder output\n",
    "        out1 = self.attention_mask(input_,keys,values, decoder_valid_lens)\n",
    "        out1 = self.add_norm(input_, out1)\n",
    "        # encoder, decoder attention\n",
    "        # (bs, num_steps, num_hidden)\n",
    "        out2 = self.attention2(out1, ith_state, ith_state, valid_len)\n",
    "        out2 = self.add_norm2(out1,out2)\n",
    "        return self.add_norm3(out2, self.ffn(out2)), ith_state, valid_len, state_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.625566Z",
     "iopub.status.busy": "2022-06-10T15:14:31.624099Z",
     "iopub.status.idle": "2022-06-10T15:14:31.643276Z",
     "shell.execute_reply": "2022-06-10T15:14:31.642505Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.625522Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_words,query_size, key_size, value_size, num_heads, num_hiddens, \n",
    "                norm_shape, num_input_size, num_hidden_size, num_output_size, num_layers,\n",
    "                dropout_rate, bias=False, **kwargs):\n",
    "        '''\n",
    "        input params:\n",
    "            num_Words: number of words in dic\n",
    "            query_size: query input dim\n",
    "            key_size: key input dim\n",
    "            value_size: value input dim\n",
    "            num_heads: number of attention heads\n",
    "            num_hidden: number of hidden size per each attention projection\n",
    "            norm_shape: normalize shape of layer normalization\n",
    "            num_input_size: input dim of ffn\n",
    "            num_hidden_size: hidden dim of ffn\n",
    "            num_output_size: output dim of ffn\n",
    "            num_layers: How many encoder blocks\n",
    "            dropout_rate: drop out rate\n",
    "            bias: whether use bias\n",
    "        '''\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        # token embedding\n",
    "        # to save\n",
    "        self.attention_weight = None\n",
    "        self.embedding_matrix = nn.Embedding(num_words, num_hiddens)\n",
    "        # positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(num_hiddens, dropout_rate)\n",
    "        self.decoder_blk = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.decoder_blk.add_module(\n",
    "            \"Decoderblock\" + str(i), Decoder(query_size, key_size, value_size, num_heads,\n",
    "                                             num_hiddens, norm_shape, num_input_size, num_hidden_size,\n",
    "                                             num_output_size,dropout_rate, i,bias))\n",
    "        # calculate all the possible output (as n classification)\n",
    "        self.ln = nn.Linear(num_hiddens, num_words)\n",
    "    def init_state(self, encoder_output, encoder_valid_lens):\n",
    "        return encoder_output, encoder_valid_lens, [None]*self.num_layers\n",
    "    \n",
    "    def forward(self, input_, state, valid_len, state_ls):\n",
    "        input_ = self.positional_encoding(self.embedding_matrix(input_)*math.sqrt(self.num_hiddens))\n",
    "        # record two multi head attention weight for each layer \n",
    "        self.attention_weight = [[None] * len(self.decoder_blk) for _ in range(2)]\n",
    "        for i, blk in enumerate(self.decoder_blk):\n",
    "            input_,state, valid_len, state_ls = blk(input_, state, valid_len, state_ls) \n",
    "            # record decoder masked self attention weight\n",
    "            self.attention_weight[0][i] = blk.attention_mask.attention.attention_weight\n",
    "            # Record encoder-decoder attention weight\n",
    "            self.attention_weight[1][i] = blk.attention2.attention.attention_weight\n",
    "        return self.ln(input_), state, valid_len, state_ls\n",
    "    \n",
    "    def return_attention_weight(self):\n",
    "        return self.attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine to fully transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.644983Z",
     "iopub.status.busy": "2022-06-10T15:14:31.644438Z",
     "iopub.status.idle": "2022-06-10T15:14:31.655316Z",
     "shell.execute_reply": "2022-06-10T15:14:31.654301Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.644944Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(Transformer, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, encoder_input, encoder_valid_lens,decoder_input):\n",
    "        encoder_output = self.encoder(encoder_input,encoder_valid_lens)\n",
    "        decoder_init_state, valid_len, state_ls = self.decoder.init_state(encoder_output,\n",
    "                                                                         encoder_valid_lens)\n",
    "        return self.decoder(decoder_input,decoder_init_state, valid_len, state_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.658727Z",
     "iopub.status.busy": "2022-06-10T15:14:31.657878Z",
     "iopub.status.idle": "2022-06-10T15:14:31.666665Z",
     "shell.execute_reply": "2022-06-10T15:14:31.665921Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.658693Z"
    }
   },
   "outputs": [],
   "source": [
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失函数\"\"\"\n",
    "    # pred的形状：(batch_size,num_steps,vocab_size)\n",
    "    # label的形状：(batch_size,num_steps)\n",
    "    # valid_len的形状：(batch_size,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        #print(pred.shape, label.shape, valid_len.shape)\n",
    "        weights = torch.ones_like(label)\n",
    "        max_len = weights.size(1)\n",
    "        mask = torch.arange((max_len), dtype = torch.float32,\n",
    "                           device = weights.device)[None,:] < valid_len[:,None]\n",
    "        weights[~mask] = 0\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "            pred.permute(0, 2, 1), label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.668303Z",
     "iopub.status.busy": "2022-06-10T15:14:31.667621Z",
     "iopub.status.idle": "2022-06-10T15:14:31.678653Z",
     "shell.execute_reply": "2022-06-10T15:14:31.677804Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.668268Z"
    }
   },
   "outputs": [],
   "source": [
    "def clip_grad(net, theta):\n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for para in params:\n",
    "            para.grad[:] *= theta/norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.680402Z",
     "iopub.status.busy": "2022-06-10T15:14:31.679974Z",
     "iopub.status.idle": "2022-06-10T15:14:31.688112Z",
     "shell.execute_reply": "2022-06-10T15:14:31.687327Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.680365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 24])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #print(masked_attention_softmax(torch.rand(2,2,4), torch.tensor([[1,3],[2,4]])))\n",
    "# query = torch.normal(0,1,(2,1,2))\n",
    "# keys = torch.ones((2,10,2))\n",
    "# #第一个 mask 2， 第二个 mask 6\n",
    "# valid_lens = torch.tensor([2, 6])\n",
    "# values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(\n",
    "#     2, 1, 1)\n",
    "# attention = Dot_product_attention(dropout_rate= 0.5)\n",
    "# attention.eval()\n",
    "# #print(attention(query, keys, values, valid_lens))\n",
    "# num_hiddens, num_heads = 100,5\n",
    "# attention = Multi_head_attention(num_hiddens, num_hiddens, num_hiddens, num_hiddens,\n",
    "#                                 num_heads, 0,5)\n",
    "# attention.eval()\n",
    "# batch_size, num_queries, num_kvpairs, valid_lens = 2, 4, 6, torch.tensor([3, 2])\n",
    "# X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "# Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "# attention(X, Y, Y, valid_lens).shape\n",
    "# ffn = FFN(4, 4, 8)\n",
    "# ffn.eval()\n",
    "# ffn(torch.ones((2, 3, 4)))\n",
    "# add_norm = Add_and_norm([3, 4], 0.5) # Normalized_shape is input.size()[1:]\n",
    "# add_norm.eval()\n",
    "# add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape\n",
    "# x = torch.ones((2,100,24))\n",
    "# valid_len = torch.tensor([3,2])\n",
    "# encoder_blk = Encoder(24, 24, 24, 8, 24,[100,24], 24,48,24,0.5)\n",
    "# encoder_blk.eval()\n",
    "# encoder_blk(x, valid_len).shape\n",
    "# encoder = TransformerEncoder(200, 24, 24,24,8,24,[100,24], 24,48,24,2,0.5)\n",
    "# encoder.eval()\n",
    "# encoder(torch.ones((2, 100), dtype=torch.long), valid_len).shape\n",
    "# decoderblk = Decoder(24, 24, 24, 8, 24,[100,24], 24,48,24,0.5,0)\n",
    "# decoderblk.eval()\n",
    "# X = torch.ones((2,100,24))\n",
    "# ith_state = encoder_blk(X, valid_lens)\n",
    "# decoderblk(X, ith_state, valid_lens,[None])[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:31.690102Z",
     "iopub.status.busy": "2022-06-10T15:14:31.689336Z",
     "iopub.status.idle": "2022-06-10T15:14:44.400523Z",
     "shell.execute_reply": "2022-06-10T15:14:44.399618Z",
     "shell.execute_reply.started": "2022-06-10T15:14:31.690067Z"
    }
   },
   "outputs": [],
   "source": [
    "# here num of step means the fixed length of each input sentences\n",
    "num_hiddens, num_layers, dropout_rate, bs, num_step = 64, 4, 0.1, 128, 10\n",
    "lr, num_epochs, device = 2e-4, 200, torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "num_input, num_hidden, num_output, num_heads = 64, 128, 64, 4\n",
    "key_size, value_size, query_size = 64,64,64\n",
    "norm_shape = [64]\n",
    "train_iter, english_mapping, french_mapping =processing_french_english_dataset(bs,num_step)\n",
    "encoder = TransformerEncoder(len(english_mapping),query_size,key_size, value_size,\n",
    "                            num_heads, num_hiddens, norm_shape, num_input, num_hidden, \n",
    "                             num_output,num_layers, dropout_rate, True)\n",
    "decoder = TransformerDecoder(len(french_mapping),query_size,key_size, value_size,\n",
    "                            num_heads, num_hiddens, norm_shape, num_input, num_hidden, \n",
    "                             num_output,num_layers, dropout_rate, True)\n",
    "transformer = Transformer(encoder, decoder)\n",
    "criterion = MaskedSoftmaxCELoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr = lr)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer= optimizer, num_warmup_steps = 0, \n",
    "                                                num_training_steps= len(train_iter), num_cycles = 0.5)\n",
    "num_gpu = 1\n",
    "max_grad_norm = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:44.402622Z",
     "iopub.status.busy": "2022-06-10T15:14:44.402052Z",
     "iopub.status.idle": "2022-06-10T15:14:44.418313Z",
     "shell.execute_reply": "2022-06-10T15:14:44.417366Z",
     "shell.execute_reply.started": "2022-06-10T15:14:44.402584Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, train_iter,criterion, optimizer, epochs, scheduler, gradient_accumulate_step, max_grad_norm ,num_gpu,\n",
    "        target_vocab):\n",
    "    net.train()   \n",
    "    # instantiate a scalar object \n",
    "    ls          = []\n",
    "    #device_ids  = [try_gpu(i) for i in range(num_gpu)]\n",
    "    device  = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    print(\"\\ntrain on %s\\n\"%str(device))\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        for idx, value in enumerate(train_iter):\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in value]\n",
    "            # when forward process, use amp\n",
    "            bos = torch.tensor([target_vocab[\"<bos>\"]]*Y.shape[0], device = device).reshape(-1,1)\n",
    "            # this called teacher forcing\n",
    "            decoder_input = torch.cat([bos, Y[:,:-1]], 1)\n",
    "            output,_,_,_  = net(X, X_valid_len, decoder_input)\n",
    "            # calculate masked loss\n",
    "            loss        = criterion(output, Y, Y_valid_len)\n",
    "            # prevent gradient to 0\n",
    "            if gradient_accumulate_step > 1:\n",
    "                # 如果显存不足，通过 gradient_accumulate 来解决\n",
    "                loss    = loss/gradient_accumulate_step\n",
    "\n",
    "            loss.sum().backward()\n",
    "            # do the gradient clip\n",
    "            gradient_norm = nn.utils.clip_grad_norm_(net.parameters(),max_grad_norm)\n",
    "            clip_grad(net, 1)\n",
    "            if (idx + 1) % gradient_accumulate_step == 0:\n",
    "                # 多少 step 更新一次梯度\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                #print(\"done 1 train\")\n",
    "            # 每1000次计算 print 出一次loss\n",
    "            if idx % 30 == 0 or idx == len(train_iter) -1:\n",
    "                with torch.no_grad():\n",
    "                    print(\"==============Epochs \"+ str(epoch) + \" ======================\")\n",
    "                    print(\"loss: \" + str(loss.mean()) + \"; grad_norm: \" + str(gradient_norm))\n",
    "                    ls.append(loss.mean().item())\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': net.state_dict(),\n",
    "                    'loss': ls\n",
    "                },\"./checkpoint.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:44.420629Z",
     "iopub.status.busy": "2022-06-10T15:14:44.419928Z",
     "iopub.status.idle": "2022-06-10T15:14:44.429160Z",
     "shell.execute_reply": "2022-06-10T15:14:44.428254Z",
     "shell.execute_reply.started": "2022-06-10T15:14:44.420591Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train(transformer, train_iter, criterion,optimizer, num_epochs, scheduler,1,max_grad_norm,\n",
    "#      num_gpu,french_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of Seq2Seq model\n",
    "Notice that, each decoder time step will take the prediction token from the previous time step and fed into the decoder as an input. Also, the initial time step is \\<bos>, when \\<eos> is predicted, then complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:44.431190Z",
     "iopub.status.busy": "2022-06-10T15:14:44.430664Z",
     "iopub.status.idle": "2022-06-10T15:14:49.609806Z",
     "shell.execute_reply": "2022-06-10T15:14:49.609034Z",
     "shell.execute_reply.started": "2022-06-10T15:14:44.431157Z"
    }
   },
   "outputs": [],
   "source": [
    "transformer.load_state_dict(torch.load(\"../input/french-english-transformer-weights/checkpoint.params\")[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:14:49.611690Z",
     "iopub.status.busy": "2022-06-10T15:14:49.611151Z",
     "iopub.status.idle": "2022-06-10T15:14:49.622946Z",
     "shell.execute_reply": "2022-06-10T15:14:49.621956Z",
     "shell.execute_reply.started": "2022-06-10T15:14:49.611653Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(net, source_sentence, source_dic_mapping, target_dic_mapping, num_steps,\n",
    "           save_attention_weight = False):\n",
    "    net.eval()\n",
    "    device  = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    net.to(device)\n",
    "    # first process the source sentence into input token part\n",
    "    source_tokens = source_dic_mapping[source_sentence.lower().split(\" \")] + [source_dic_mapping[\"<eos>\"]]\n",
    "    encoder_valid_len = torch.tensor([len(source_tokens)], device = device)\n",
    "    # padding \n",
    "    source_tokens = padding_truncation(source_tokens, num_steps, source_dic_mapping[\"<pad>\"])\n",
    "    # create batch dimension of input\n",
    "    encoder_input = torch.unsqueeze(torch.tensor(source_tokens, dtype = torch.long, device = device), dim = 0)\n",
    "    encoder_output = net.encoder(encoder_input, encoder_valid_len)\n",
    "    state, valid_len, state_ls = net.decoder.init_state(encoder_output,encoder_valid_len)\n",
    "    # create decoder input, first just the <bos> special token\n",
    "    decoder_input = torch.unsqueeze(torch.tensor([target_dic_mapping[\"<bos>\"]], dtype = torch.long, device = device), dim = 0)\n",
    "    #print(decoder_input.shape)\n",
    "    # for loop on time steps dimension\n",
    "    output_seq, att_weight_seq = [], []\n",
    "    for i in range(num_steps):\n",
    "        decoder_output, state, valid_len, state_ls = net.decoder(decoder_input, state, valid_len, state_ls)\n",
    "        # in the next time step, use prediction result as the next input state\n",
    "        decoder_input = decoder_output.argmax(dim = 2)\n",
    "        # reduce batch size\n",
    "        pred = decoder_input.squeeze(dim = 0).type(torch.int32).item()\n",
    "        if pred == target_dic_mapping[\"<eos>\"]:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return \" \".join(target_dic_mapping.indices_to_tokens(output_seq)), att_weight_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The metric of evaluation of machine-translation problem\n",
    "In general, a metric called `BLEU` (Bilingual Evaluation Understudy) is used for this task, defined as following:\n",
    "1. Denote $p_n$, precision of n-grams, which is the ration of the number of matched n-grams in the predicted and label sequences to the number of n-grams in the predicted sequence, 即其为两个数量的比值，第一个为预测序列与标签序列中匹配的n-gram数量，第二个是预测序列中n-gram数量的比率， 比如 label: A, B, C, D, E,F; Predict: A,B,B,C,D,对于：\n",
    "    1. 1-gram, 共有5组，有四个是对的，即 \\<BOS>->A, A->B, B->B, B->C, C->D, 其中和label匹配的有\\<BOS>->A, A->B, B->C, C->D, 所以此处的precision为 4/5\n",
    "    2. 2-gram, 一共4组，其中三个是对的，precision为3/4，以此类推\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Formula here is:\n",
    "$exp(min(0,1-\\frac{len(label)}{len(pred)}))\\prod_{n=1}^{k}p_n^{1/2^n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on definition of BLEU, it's obvious to find that the longer the n-gram, the more difficult to match. When pn is fixed, the longer n, the larger the value. Also, the shorter, the larger score, so, we use a exp to punish this situation, which will punish the shorter sequence output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:15:13.451076Z",
     "iopub.status.busy": "2022-06-10T15:15:13.450441Z",
     "iopub.status.idle": "2022-06-10T15:15:13.459417Z",
     "shell.execute_reply": "2022-06-10T15:15:13.458644Z",
     "shell.execute_reply.started": "2022-06-10T15:15:13.451024Z"
    }
   },
   "outputs": [],
   "source": [
    "def BLEU(pred, label, n_gram):\n",
    "    assert isinstance(pred, str)\n",
    "    assert isinstance(label, str)\n",
    "    pred_tokens, labels_tokens = pred.split(\" \"), label.split(\" \")\n",
    "    len_prediction, len_label = len(pred_tokens), len(labels_tokens)\n",
    "    score = math.exp(min(0, 1 - len(label)/len(pred)))\n",
    "    for n in range(1,n_gram +1):\n",
    "        num_matches, label = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label[\" \".join(labels_tokens[i:i+n])] += 1\n",
    "        for i in range(len_prediction - n + 1):\n",
    "            if label[\" \".join(pred_tokens[i:i+n])] > 0:\n",
    "                num_matches += 1\n",
    "                label[\" \".join(pred_tokens[i:i+n])]-=1\n",
    "        score *= math.pow(num_matches/(len_prediction - n + 1), math.pow(0.5,n))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-10T15:15:13.920004Z",
     "iopub.status.busy": "2022-06-10T15:15:13.919680Z",
     "iopub.status.idle": "2022-06-10T15:15:14.069592Z",
     "shell.execute_reply": "2022-06-10T15:15:14.068120Z",
     "shell.execute_reply.started": "2022-06-10T15:15:13.919976Z"
    }
   },
   "outputs": [],
   "source": [
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, dec_attention_weight_seq = predict(\n",
    "        transformer, eng, english_mapping, french_mapping, num_step)\n",
    "    print(str(eng) +\"===>\"+str(translation))\n",
    "    print(\"BLEU_SCORE: \", BLEU(translation, fra, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
